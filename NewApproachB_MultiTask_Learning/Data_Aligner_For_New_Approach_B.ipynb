{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZCkbmHItdq8"
   },
   "source": [
    "## **New Approch B: Data Alignment & Feature Extraction**\n",
    "\n",
    "### **Objective**\n",
    "This notebook addresses the critical data misalignment issue discovered in `pipeline2_visual_descriptors.py`.\n",
    "\n",
    "**Context in Pipeline:**\n",
    "1.  **Previous Step:** `prepare_visual_metadata.py` generated a raw, unordered CSV of species metadata (`full_visual_metadata.csv`).\n",
    "2.  **This Step:** We read the official dataset order (`train.txt`) and force our metadata to match that exact sequence. We also generate frozen DINOv2 embeddings to serve as a baseline.\n",
    "3.  **Next Step:** The output files from this notebook (`aligned_train_metadata.pkl`) will be fed into the LoRA training script.\n",
    "\n",
    "**Outputs:**\n",
    "*   `aligned_train_metadata.pkl`: The master dataframe, perfectly ordered to match the image directory.\n",
    "*   `aligned_train_embeddings.npy`: Pre-computed DINOv2 features (used for baseline comparison)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zp8wFb5otubX"
   },
   "source": [
    "# **1.0 Environment Setup**\n",
    "We mount Google Drive to access the dataset and install the necessary libraries (`timm` for the model, `pandas` for data manipulation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fc-eq644tvo_"
   },
   "source": [
    "## **1.1 Mount Google Drive**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1782,
     "status": "ok",
     "timestamp": 1763046128548,
     "user": {
      "displayName": "Brenda Sim",
      "userId": "07076135971196410632"
     },
     "user_tz": -480
    },
    "id": "54_lKkN1nOOy",
    "outputId": "79901b1a-2b5c-43a6-ebf4-a40f86406839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "✅ Google Drive mounted successfully.\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"✅ Google Drive mounted successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqyV1VBOtyk2"
   },
   "source": [
    "## **1.2 Install Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11909,
     "status": "ok",
     "timestamp": 1763046140456,
     "user": {
      "displayName": "Brenda Sim",
      "userId": "07076135971196410632"
     },
     "user_tz": -480
    },
    "id": "ARmU9mbYt05J",
    "outputId": "a81c0a8f-b4bf-4343-d606-e058ee15eded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Required libraries are installed.\n"
     ]
    }
   ],
   "source": [
    "# Install all necessary libraries. The '-q' flag makes the output less verbose.\n",
    "!pip install pandas scikit-learn torch torchvision tqdm matplotlib timm kagglehub -q\n",
    "\n",
    "print(\"✅ Required libraries are installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqWTF2qYt3rX"
   },
   "source": [
    "## **1.3 Define Project Paths and Constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2909,
     "status": "ok",
     "timestamp": 1763046143368,
     "user": {
      "displayName": "Brenda Sim",
      "userId": "07076135971196410632"
     },
     "user_tz": -480
    },
    "id": "ceit-YY3t5J1",
    "outputId": "5045241f-05e8-4d3c-bad6-edb298d139cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Path: /content/drive/My Drive/Colab Notebooks/COS30082_ML_Project\n",
      "Dataset Path: /content/drive/My Drive/Colab Notebooks/COS30082_ML_Project/AML_dataset\n",
      "Using device: cpu\n",
      "Kaggle cache path set to: /content/drive/My Drive/Colab Notebooks/COS30082_ML_Project/kaggle_cache\n"
     ]
    }
   ],
   "source": [
    "# This cell centralizes all configuration for easy access.\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# 1. Define the base path to project folder in Google Drive.\n",
    "PROJECT_PATH = '/content/drive/My Drive/Colab Notebooks/COS30082_ML_Project'\n",
    "\n",
    "# 2. Set the device for processing.\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 3. Define paths to key data files.\n",
    "DATA_DIR = os.path.join(PROJECT_PATH, 'AML_dataset')\n",
    "METADATA_FILE = os.path.join(PROJECT_PATH, 'full_visual_metadata.csv')\n",
    "\n",
    "# 4. Set up the persistent Kaggle cache for the DINOv2 model.\n",
    "cache_dir = os.path.join(PROJECT_PATH, 'kaggle_cache')\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "os.environ['KAGGLE_CACHE'] = cache_dir\n",
    "\n",
    "print(f\"Project Path: {PROJECT_PATH}\")\n",
    "print(f\"Dataset Path: {DATA_DIR}\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Kaggle cache path set to: {cache_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPJ9l0oAt9TC"
   },
   "source": [
    "# **2.0 Load the DINOv2 Feature Extractor**\n",
    "To verify that all images are readable and to generate baseline features, we load the specific DINOv2 model used in previous baselines. We ensure the classifier head is removed so we get raw feature vectors (size 768).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5565,
     "status": "ok",
     "timestamp": 1763046296298,
     "user": {
      "displayName": "Brenda Sim",
      "userId": "07076135971196410632"
     },
     "user_tz": -480
    },
    "id": "jlvT0WU0t9rv",
    "outputId": "2743bcba-2c9a-4140-8b75-a47eb380c578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading DINOv2 Model ---\n",
      "Found weights file: model_best.pth.tar\n",
      "✅ DINOv2 model ready and moved to device.\n",
      "✅ Image transformer ready.\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import timm\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "print(\"--- Loading DINOv2 Model ---\")\n",
    "try:\n",
    "    path_to_model_files = kagglehub.model_download(\"juliostat/dinov2_patch14_reg4_onlyclassifier_then_all/pyTorch/default\")\n",
    "    timm_model_name = 'vit_base_patch14_reg4_dinov2.lvd142m'\n",
    "    dinov2_model = timm.create_model(timm_model_name, pretrained=False)\n",
    "    dinov2_model.reset_classifier(0, '')\n",
    "\n",
    "    weights_filename = next((f for f in os.listdir(path_to_model_files) if f.endswith(('.pth', '.pt', '.tar'))), None)\n",
    "    if weights_filename is None:\n",
    "        raise FileNotFoundError(\"Could not find a model weights file in the cache directory.\")\n",
    "\n",
    "    weights_path = os.path.join(path_to_model_files, weights_filename)\n",
    "    print(f\"Found weights file: {weights_filename}\")\n",
    "\n",
    "    # Use weights_only=False as required by this specific checkpoint file\n",
    "    checkpoint = torch.load(weights_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "    state_dict = checkpoint.get('state_dict', checkpoint)\n",
    "    dinov2_model.load_state_dict(state_dict, strict=False)\n",
    "    dinov2_model.to(DEVICE)\n",
    "    dinov2_model.eval()\n",
    "    print(\"✅ DINOv2 model ready and moved to device.\")\n",
    "\n",
    "    # Define the required image transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(518, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(518),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    print(\"✅ Image transformer ready.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to load DINOv2 model. Error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnuJuMsLuH2F"
   },
   "source": [
    "# **3.0 Generate and Save Aligned Data**\n",
    "This is the **most critical step**. \n",
    "The `full_visual_metadata.csv` file we created earlier is likely sorted by ID or Name. \n",
    "However, the dataset expects images to be loaded in the order defined by `list/train.txt`.\n",
    "\n",
    "We load `train.txt` first to establish the \"Ground Truth\" order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6355,
     "status": "ok",
     "timestamp": 1763046320257,
     "user": {
      "displayName": "Brenda Sim",
      "userId": "07076135971196410632"
     },
     "user_tz": -480
    },
    "id": "D7I0UNdtuLD6",
    "outputId": "788ee095-e832-4b80-91c4-d7ce4760e331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Reading Official File Order ---\n",
      "Found 4744 images to process.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Step 1: Get the OFFICIAL file order from train.txt ---\n",
    "print(\"--- Reading Official File Order ---\")\n",
    "train_list_path = os.path.join(DATA_DIR, 'list/train.txt')\n",
    "train_order_df = pd.read_csv(train_list_path, sep=' ', header=None, names=['filepath', 'classid'])\n",
    "print(f\"Found {len(train_order_df)} images to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "9563ed6089fe46feb753b0f2408d3920",
      "f60af913cc68457ea03319a18a9dd762",
      "cf1560506fe848a0b912d074b4f875dc",
      "7f850af9f85e4063b2e0939657a5541e",
      "7d720f9b89c448809a0157b65262cde4",
      "6463dcc3a360432380422c4e284264b9",
      "5e58d7c075a74c4bb5cde00f95491be3",
      "8acdc77de3e24cb38292b1b84bf13832",
      "e656240abe8d4f728a133853f7f93d3f",
      "da5ff075ace9446f9341ef65199e7bde",
      "334fb9cb215a433999df670027e304b6"
     ]
    },
    "executionInfo": {
     "elapsed": 28086667,
     "status": "ok",
     "timestamp": 1763074436378,
     "user": {
      "displayName": "Brenda Sim",
      "userId": "07076135971196410632"
     },
     "user_tz": -480
    },
    "id": "rX84m-VJuNkG",
    "outputId": "5f6b3fd4-a911-4b39-9466-ce881fa2b84b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Embeddings in Correct Order ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9563ed6089fe46feb753b0f2408d3920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting Features:   0%|          | 0/4744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# --- Step 2: Process images IN ORDER and collect embeddings ---\n",
    "aligned_embeddings = []\n",
    "print(\"\\n--- Generating Embeddings in Correct Order ---\")\n",
    "\n",
    "# Loop through the official list of training files\n",
    "for index, row in tqdm(train_order_df.iterrows(), total=len(train_order_df), desc=\"Extracting Features\"):\n",
    "    image_path = os.path.join(DATA_DIR, row['filepath'])\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            full_features = dinov2_model(img_tensor)\n",
    "            embedding = full_features[:, 0, :].cpu().numpy().flatten()\n",
    "        aligned_embeddings.append(embedding)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Could not find file {image_path}. Skipping.\")\n",
    "        # We assume all files exist as per the dataset structure.\n",
    "        # If files were missing, this would need error handling (e.g., adding a placeholder).\n",
    "        continue\n",
    "\n",
    "# Convert the list of embeddings to a single NumPy array\n",
    "final_embeddings_array = np.array(aligned_embeddings, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 610,
     "status": "ok",
     "timestamp": 1763074437053,
     "user": {
      "displayName": "Brenda Sim",
      "userId": "07076135971196410632"
     },
     "user_tz": -480
    },
    "id": "vZxlkByEuPlc",
    "outputId": "c3cc14ba-e097-48a4-85ea-1997f065e3b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aligning Full Metadata ---\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Create the aligned metadata DataFrame ---\n",
    "print(\"\\n--- Aligning Full Metadata ---\")\n",
    "df_unordered = pd.read_csv(METADATA_FILE)\n",
    "# Normalize paths for a successful merge\n",
    "df_unordered['filepath'] = df_unordered['filepath'].str.replace('\\\\', '/')\n",
    "\n",
    "# Merge the full metadata onto our correctly ordered DataFrame.\n",
    "# This enforces the order of `train_order_df`.\n",
    "aligned_metadata_df = pd.merge(train_order_df, df_unordered, on=['filepath', 'classid'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1763074437070,
     "user": {
      "displayName": "Brenda Sim",
      "userId": "07076135971196410632"
     },
     "user_tz": -480
    },
    "id": "j1_0Lu_puRAv",
    "outputId": "5ef58ec8-200d-46e1-872f-2256b3d60845"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ✅ SUCCESS! ---\n",
      "Final Embeddings Shape: (4744, 768)\n",
      "Final Metadata Shape:   (4744, 9)\n",
      "\n",
      "Saved aligned embeddings to: /content/drive/My Drive/Colab Notebooks/COS30082_ML_Project/aligned_train_embeddings.npy\n",
      "Saved aligned metadata to:   /content/drive/My Drive/Colab Notebooks/COS30082_ML_Project/aligned_train_metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Save the new, perfectly aligned files ---\n",
    "EMBEDDINGS_SAVE_PATH = os.path.join(PROJECT_PATH, 'aligned_train_embeddings.npy')\n",
    "METADATA_SAVE_PATH = os.path.join(PROJECT_PATH, 'aligned_train_metadata.pkl')\n",
    "\n",
    "np.save(EMBEDDINGS_SAVE_PATH, final_embeddings_array)\n",
    "aligned_metadata_df.to_pickle(METADATA_SAVE_PATH)\n",
    "\n",
    "print(\"\\n--- ✅ SUCCESS! ---\")\n",
    "print(f\"Final Embeddings Shape: {final_embeddings_array.shape}\")\n",
    "print(f\"Final Metadata Shape:   {aligned_metadata_df.shape}\")\n",
    "print(f\"\\nSaved aligned embeddings to: {EMBEDDINGS_SAVE_PATH}\")\n",
    "print(f\"Saved aligned metadata to:   {METADATA_SAVE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "334fb9cb215a433999df670027e304b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e58d7c075a74c4bb5cde00f95491be3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6463dcc3a360432380422c4e284264b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d720f9b89c448809a0157b65262cde4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f850af9f85e4063b2e0939657a5541e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da5ff075ace9446f9341ef65199e7bde",
      "placeholder": "​",
      "style": "IPY_MODEL_334fb9cb215a433999df670027e304b6",
      "value": " 4744/4744 [7:48:06&lt;00:00,  5.80s/it]"
     }
    },
    "8acdc77de3e24cb38292b1b84bf13832": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9563ed6089fe46feb753b0f2408d3920": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f60af913cc68457ea03319a18a9dd762",
       "IPY_MODEL_cf1560506fe848a0b912d074b4f875dc",
       "IPY_MODEL_7f850af9f85e4063b2e0939657a5541e"
      ],
      "layout": "IPY_MODEL_7d720f9b89c448809a0157b65262cde4"
     }
    },
    "cf1560506fe848a0b912d074b4f875dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8acdc77de3e24cb38292b1b84bf13832",
      "max": 4744,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e656240abe8d4f728a133853f7f93d3f",
      "value": 4744
     }
    },
    "da5ff075ace9446f9341ef65199e7bde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e656240abe8d4f728a133853f7f93d3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f60af913cc68457ea03319a18a9dd762": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6463dcc3a360432380422c4e284264b9",
      "placeholder": "​",
      "style": "IPY_MODEL_5e58d7c075a74c4bb5cde00f95491be3",
      "value": "Extracting Features: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
